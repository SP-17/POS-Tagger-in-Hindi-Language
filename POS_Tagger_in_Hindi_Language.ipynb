{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IrzHCJFyqkDt"
      },
      "outputs": [],
      "source": [
        "# Importing NLTK library, corpus and tagger\n",
        "import nltk\n",
        "from nltk.corpus import indian\n",
        "from nltk.tag import tnt\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading Indian Languages Corpora which consists Hindi, Bangla, Marathi and Telugu corpus respectively\n",
        "nltk.download(\"indian\")\n",
        "\n",
        "# Downloading Tokenizers\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hlx--jZjt6CX",
        "outputId": "709ee460-98d8-4a4e-d3e6-5879e5c3558b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/indian.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the POS Tagger Model using Hindi dataset\n",
        "def train():\n",
        "    taggedSet = \"hindi.pos\"\n",
        "    wordSet = indian.sents(taggedSet)\n",
        "    count = 0\n",
        "\n",
        "    # Joining dataset words to form a sentence\n",
        "    for sen in wordSet:\n",
        "        count = count + 1\n",
        "        sen = \"\".join(\n",
        "            [\n",
        "                \" \" + i if not i.startswith(\"'\") and i not in string.punctuation else i\n",
        "                for i in sen\n",
        "            ]\n",
        "        ).strip()\n",
        "        # Viewing individual corpus sentence\n",
        "        # print(count, sen, \"sentences\")\n",
        "\n",
        "    # Total Sentence Count\n",
        "    print(\"Total sentences in the tagged file are\", count)\n",
        "\n",
        "    # Spliting dataset into Training Data and Test Data\n",
        "    trainPerc = 0.9\n",
        "\n",
        "    # Assigning the last index for Training Data and first index of Test Data\n",
        "    trainRows = int(trainPerc * count)\n",
        "    testRows = trainRows + 1\n",
        "\n",
        "    # Slicing the corpus\n",
        "    data = indian.tagged_sents(taggedSet)\n",
        "    train_data = data[:trainRows]\n",
        "    test_data = data[testRows:]\n",
        "\n",
        "    # Stats\n",
        "    print(\"Training dataset length: \", len(train_data))\n",
        "    print(\"Testing dataset length: \", len(test_data))\n",
        "    pos_tagger = tnt.TnT()\n",
        "    pos_tagger.train(train_data)\n",
        "    print(\"Accuracy: \", pos_tagger.evaluate(test_data))\n",
        "\n",
        "    return pos_tagger"
      ],
      "metadata": {
        "id": "L2yqNBl8uADi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tagging function to tag all words in a sentence\n",
        "def tagger(pos_tagger, sentenceToBeTagged):\n",
        "    tokenized = nltk.word_tokenize(sentenceToBeTagged)\n",
        "    return pos_tagger.tag(tokenized)"
      ],
      "metadata": {
        "id": "hfI4hnaYuDow"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Driving Module\n",
        "if __name__ == \"__main__\":\n",
        "    pos_tagger = train()\n",
        "    sentence_to_be_tagged = \"प्रधानमंत्री की सिफारिश पर मंत्रियों के एक पैनल का गठन किया गया था ।\"\n",
        "    print(tagger(pos_tagger, sentence_to_be_tagged))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMS7ujwPuEUf",
        "outputId": "2babfeb9-cca7-4f31-aba6-d327acb68114"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences in the tagged file are 540\n",
            "Training dataset length:  486\n",
            "Testing dataset length:  53\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-537f56bcd405>:39: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  print(\"Accuracy: \", pos_tagger.evaluate(test_data))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.8111964873765093\n",
            "[('प्रधानमंत्री', 'NN'), ('की', 'PREP'), ('सिफारिश', 'NN'), ('पर', 'PREP'), ('मंत्रियों', 'NN'), ('के', 'PREP'), ('एक', 'QFNUM'), ('पैनल', 'NN'), ('का', 'PREP'), ('गठन', 'NVB'), ('किया', 'VFM'), ('गया', 'VAUX'), ('था', 'VAUX'), ('।', 'PUNC')]\n"
          ]
        }
      ]
    }
  ]
}